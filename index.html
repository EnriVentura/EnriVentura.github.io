<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Enrico Ventura's Personal Page</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <h1>Enrico Ventura</h1>
        <p>PostDoctoral Researcher</p>
        <p>enricoventura.pm@gmail.com</p>
    </header>
    <div class="container">
        <section id="about" class="section about-section">
            <img src="profile.jpeg" alt="Enrico Ventura" class="profile-image">
            <div class="about-text">
                <h2>Ciao :)</h2>
                <p>I'm a PostDoc at Bocconi Institute of Data Science and Analytics (BIDSA) in Milan, Italy. My research investigates the Statistical Mechanics of Disordered Systems and its multidisciplinary applications.</p>
                <p>At the moment I'm interested in finding new learning paradigms at the interface between Artificial Intelligence and Biology.</p>
            </div>
        </section>
        <div class="container">
            <div class="flex-container">
                <section id="background" class="section">
                    <h2>Academic Background (full CV <a href="https://drive.google.com/file/d/1zzAEgJ2P4Y-Qurwf-3jA0G6quJhMZLkS/view?usp=sharing">here</a>)</h2>
                    <ul>
                        <li> 2024 - Present: PostDoc in Physics and Computer Science, Bocconi University.<br> 
                            Supervisor: Carlo Lucibello.</li> 
                        <li> 2020 - 2023: (Joint) Ph.D. in Theoretical Physics, La Sapienza & École Normale Supérieure.<br> 
                            Supervisors: Profs. Giancarlo Ruocco, Francesco Zamponi.<br>
                            My Ph.D. thesis is available <a href="https://arxiv.org/pdf/2403.02537">here</a>.</li>
                        <li>2018 - 2020: M.Sc. in Theoretical Physics, La Sapienza.<br>
                            I spent my second year as an Erasmus student at Sorbonne University.<br> 
                            My M.Sc. thesis is available <a href="https://arxiv.org/pdf/2305.07656">here</a>.</li>
                        <li>2015 - 2018: B.Sc. in Physics, La Sapienza.</li>
                    </ul>
                </section>    
                <section id="research" class="section">
                    <h2>Research Interests</h2>
                    <ul>
                        <li>Statistical Mechanics</li>
                        <li>Inference and Networks</li>
                        <li>Algorithms</li>
                        <li>Artificial Intelligence</li>
                        <li>Complex Systems</li>
                    </ul>
                </section>
            </div>
        </div>
        <section id="projects" class="section">
            <h2>Projects</h2>
            <ul>
                <li>
                    <h3>Glassy Physics of Diffusion Models</h3>
                    <p>Diffusion models represent the state of the art in image and video generation. These models learn the distribution of the data through a diffusive trajectory in the data-space, the same type of dynamical processes studied by stochastic thermodynamics. 
                        Statistical mechanics has recently helped at providing for crucial insights about the complex dynamics with which such models learn the data distribution.  
                        Me and my collaborators are now studying the learning dynamics in case of stractured training data, i.e. data-points that live on a low dimensional manifold.    
                        We are exploiting Random Matrix Theory and the physics of Random Energy Models to answer the following questions:
                         <ul>
                            <li> Does the structure of the data help reducing overfitting and improving generalization? </li>
                            <li> Can we infer the way the model is learning the data-structure by looking at the geometry of the diffusive trajectory? </li>
                         </ul>
                    </p>
                </li>
                <li>
                    <h3> Learning & Unlearning in Recurrent Networks</h3>
                    <p> The human brain is a complex system that is fascinating scientists since a long time. Its remarkable capabilities include classification of concepts, retrieval of memories and creative generation of new examples. At the same time, modern artificial neural networks are trained on large amounts of data to accomplish the same type of tasks with a considerable degree of precision.
By contrast with biological systems, machines appear to be either significantly slow and energetically expensive to train, suggesting the need for a paradigmatic change in statistical learning. 
We evaluate a known training procedure for recurrent neural networks that can be split into a prior Hebbian learning phase and a subsequent anti-Hebbian one (usually referred to as Unlearning). We are progressively proving that this unsupervised prescription is capable of performing classification, memorization and generation of examples with a high degree of efficacy while aligning with some modern biological theories of learning. </p>
                </li>
            </ul>
        </section>    
        <section id="publications" class="section">
            <h2>Publications (<a href="https://scholar.google.com/citations?hl=it&user=h2YpRNUAAAAJ">scholar</a>)</h2>
            <ul>
                <li><strong>E. Ventura</strong>, S. Cocco, R. Monasson, F. Zamponi (2024). Unlearning regularization for Boltzmann machines. <em>Machine Learning: Science and Technology</em>, 5(2), 025078.</li>
                <li><strong>E. Ventura</strong>, M. Benedetti. (2023). Training neural networks with structured noise improves classification and generalization. <em>arXiv:2302.13417</em>, 6.</li>
                <li><strong>E. Ventura</strong>, M. Benedetti, E. Marinari, G. Ruocco, F. Zamponi (2022). Supervised perceptron learning vs unsupervised Hebbian unlearning: Approaching optimal memory retrieval in Hopfield-like networks. <em>The Journal of Chemical Physics</em>, 156(10), 21.</li>
             </ul>
        </section>
        </section>    
            <section id="gender balance" class="section">
                <h2>Gender (in)equality in Academia</h2>
                <p>I'm active in studying and divulgating issues relative to gender discrimination and inequalities in academia, especially in the STEM environment.</p>
                <p> Here is a series of posters assembled with the <a href="https://phd.uniroma1.it/web/pagina.aspx?s=&i=3503&m=&l=EN&p=477&a=">Gender Balance Working Group</a> of La Sapienza Physics Department, obtained from the comics of <a href="https://didthisreallyhappen.net/">Did This Really Happen?!</a> and displayed at La Sapienza in Autumn 2023.</p>  
             <div class="image-row">
                <img src="Did this REALLY happen!-2.jpg">
                <img src="Did this REALLY happen!-3.jpg">
                <img src="Did this REALLY happen!-4.jpg">
            </div>
            <div class="image-row">
                <img src="Did this REALLY happen!-5.jpg">
                <img src="Did this REALLY happen!-6.jpg">
            </div>
        </section>
    </div>
</body>
</html>

